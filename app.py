# # app.py
# import os
# import time
# import pika
# from typing import Optional
# from pydantic import BaseModel
# from dotenv import load_dotenv
# from fastapi import FastAPI, Query
# #from storage import init_db, query_recommendations
# from storage import init_db, query_recommendations_paginated

# load_dotenv()

# SERVICE_NAME = "aimas-recommendation"
# VERSION = os.getenv("SERVICE_VERSION", "0.1.0")
# RABBIT_URL: Optional[str] = os.getenv("RABBIT_URL")  # e.g. amqps://user:pass@host:5671/%2Fvhost

# app = FastAPI(title="AIMAS Recommendation Service", version=VERSION)

# @app.on_event("startup")
# async def startup_event():
#     init_db()

# @app.get("/recommendations", summary="Fetch recent recommendations")
# async def get_recommendations(
#     limit: int = Query(50, description="Maximum number of recommendations to return", ge=1, le=200),
#     since: str = Query(None, description="ISO timestamp filter, optional"),
#     event_type: str = Query(None, description="Filter by event type (e.g., system.cpu)"),
# ):
#     """
#     Retrieve recent recommendations generated by the service.
#     Optionally filter by timestamp (`since`) or event type.
#     """
#     results = query_recommendations_paginated(limit=limit, since=since, event_type=event_type)
#     return {"count": len(results), "items": results}


# @app.get("/recommendations", summary="Fetch recent recommendations")
# def get_recommendations(
#     page: int = Query(1, ge=1, description="Page number (1-based)"),
#     page_size: int = Query(50, ge=1, le=200, description="Items per page"),
#     since: Optional[str] = Query(None, description="ISO timestamp filter (optional)"),
#     event_type: Optional[str] = Query(None, description="Filter by event type, e.g., system.cpu"),
# ):
#     items, total = query_recommendations_paginated(
#         page=page, page_size=page_size, since=since, event_type=event_type
#     )
#     pages = (total + page_size - 1) // page_size if page_size else 1
#     return {
#         "page": page,
#         "page_size": page_size,
#         "total": total,
#         "pages": pages,
#         "items": items,
#     }

# class HealthResponse(BaseModel):
#     status: str              # "ok" | "degraded" | "not_ready"
#     service: str
#     version: str
#     time: float
#     rabbitmq: dict


# @app.get("/health/live", response_model=HealthResponse, tags=["health"])
# def live():
#     """
#     Liveness: process is up and responsive.
#     Does NOT require RabbitMQ to be configured or reachable.
#     """
#     return HealthResponse(
#         status="ok",
#         service=SERVICE_NAME,
#         version=VERSION,
#         time=time.time(),
#         rabbitmq={
#             "configured": bool(RABBIT_URL),
#             "reachable": None,   # not checked on liveness
#             "error": None
#         },
#     )


# @app.get("/health/ready", response_model=HealthResponse, tags=["health"])
# def ready():
#     """
#     Readiness: deep check. If RABBIT_URL is provided, try to connect quickly.
#     If RABBIT_URL is missing, return 'degraded' (service is up, broker not configured yet).
#     """
#     if not RABBIT_URL:
#         return HealthResponse(
#             status="degraded",
#             service=SERVICE_NAME,
#             version=VERSION,
#             time=time.time(),
#             rabbitmq={
#                 "configured": False,
#                 "reachable": None,
#                 "error": "RABBIT_URL not configured"
#             },
#         )

#     # Try a short RabbitMQ connect/close
#     try:
#         params = pika.URLParameters(RABBIT_URL)
#         # Keep timeouts low so the endpoint returns fast
#         params.socket_timeout = 3
#         params.heartbeat = 0
#         conn = pika.BlockingConnection(params)
#         conn.close()
#         return HealthResponse(
#             status="ok",
#             service=SERVICE_NAME,
#             version=VERSION,
#             time=time.time(),
#             rabbitmq={
#                 "configured": True,
#                 "reachable": True,
#                 "error": None
#             },
#         )
#     except Exception as e:
#         return HealthResponse(
#             status="not_ready",
#             service=SERVICE_NAME,
#             version=VERSION,
#             time=time.time(),
#             rabbitmq={
#                 "configured": True,
#                 "reachable": False,
#                 "error": repr(e)
#             },
#         )



# app.py
import os
import json
import time
import pika
from dotenv import load_dotenv
from pydantic import BaseModel
from typing import Optional, List
from fastapi import FastAPI, Depends, Header, HTTPException, Query, status
from storage import init_db, query_recommendations_paginated

# Rule packs (deterministic)
from rules.cpu_rules import CPURulePack
from rules.memory_rules import MemoryRulePack
from rules.disk_rules import DiskRulePack
from rules.payment_rules import PaymentAPIRulePack
from rules.system_net_rules import SystemNetRulePack
from rules.error_rate_rules import ServiceErrorRateRulePack
from rules.network_http_rules import NetworkHttpRulePack
from rules.generic_rules import GenericRulePack

load_dotenv()

SERVICE_NAME = "aimas-recommendation"
VERSION = os.getenv("SERVICE_VERSION", "0.1.0")
RABBIT_URL: Optional[str] = os.getenv("RABBIT_URL")  # e.g. amqps://user:pass@host:5671/%2Fvhost

app = FastAPI(title="AIMAS Recommendation Service")

# -----------------------------
# DB init
# -----------------------------
@app.on_event("startup")
async def startup_event():
    init_db()

# -----------------------------
# Auth (API Key)
# -----------------------------
_API_KEYS = [k.strip() for k in os.getenv("API_KEYS", "").split(",") if k.strip()]

def require_api_key(x_api_key: Optional[str] = Header(None)):
    # Health endpoints are deliberately left open (no dependency there).
    if not _API_KEYS:
        # If API_KEYS not configured, leave endpoint open (useful in dev).
        return
    if not x_api_key or x_api_key not in _API_KEYS:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or missing API key",
        )

class HealthResponse(BaseModel):
    status: str              # "ok" | "degraded" | "not_ready"
    service: str
    version: str
    time: float
    rabbitmq: dict


@app.get("/health/live", response_model=HealthResponse, tags=["health"])
def live():
    """
    Liveness: process is up and responsive.
    Does NOT require RabbitMQ to be configured or reachable.
    """
    return HealthResponse(
        status="ok",
        service=SERVICE_NAME,
        version=VERSION,
        time=time.time(),
        rabbitmq={
            "configured": bool(RABBIT_URL),
            "error": None
        },
    )


@app.get("/health/ready", response_model=HealthResponse, tags=["health"])
def ready():
    """
    Readiness: deep check. If RABBIT_URL is provided, try to connect quickly.
    If RABBIT_URL is missing, return 'degraded' (service is up, broker not configured yet).
    """
    if not RABBIT_URL:
        return HealthResponse(
            status="degraded",
            service=SERVICE_NAME,
            version=VERSION,
            time=time.time(),
            rabbitmq={
                "configured": False,
                "reachable": None,
                "error": "RABBIT_URL not configured"
            },
        )

    # Try a short RabbitMQ connect/close
    try:
        params = pika.URLParameters(RABBIT_URL)
        # Keep timeouts low so the endpoint returns fast
        params.socket_timeout = 3
        params.heartbeat = 0
        conn = pika.BlockingConnection(params)
        conn.close()
        return HealthResponse(
            status="ok",
            service=SERVICE_NAME,
            version=VERSION,
            time=time.time(),
            rabbitmq={
                "configured": True,
                "reachable": True,
                "error": None
            },
        )
    except Exception as e:
        return HealthResponse(
            status="not_ready",
            service=SERVICE_NAME,
            version=VERSION,
            time=time.time(),
            rabbitmq={
                "configured": True,
                "reachable": False,
                "error": repr(e)
            },
        )


# -----------------------------
# Deterministic rule engine (for POST /analyze fallback)
# -----------------------------
_SPECIFIC_RULES = [
    CPURulePack(),
    MemoryRulePack(),
    DiskRulePack(),
    SystemNetRulePack(),
    PaymentAPIRulePack(),
    ServiceErrorRateRulePack(),
    NetworkHttpRulePack(),
]
_GENERIC_RULE = GenericRulePack()

def _normalize_metrics(event: dict) -> dict:
    m = (event.get("metrics") or {})
    et = event.get("type", "")
    # small compat shim
    if et in ("system.cpu", "host.cpu"):
        if "usage_pct" not in m and "used_pct" in m:
            m["usage_pct"] = m["used_pct"]
    event["metrics"] = m
    return event

def evaluate_rules(event: dict) -> List[str]:
    et = event.get("type")
    recos: List[str] = []
    for pack in _SPECIFIC_RULES:
        if pack.supports(et):
            recos += pack.evaluate(event)
    if not recos:
        recos += _GENERIC_RULE.evaluate(event)
    # de-dup
    return list(dict.fromkeys(recos))

# -----------------------------
# Optional LLM support (for POST /analyze)
# -----------------------------
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_TEMPERATURE = float(os.getenv("OPENAI_TEMPERATURE", "0.2"))
OPENAI_MAX_TOKENS = int(os.getenv("OPENAI_MAX_TOKENS", "400"))
USE_LLM = bool(OPENAI_API_KEY)

_OPENAI_CLIENT = None
def _ensure_openai():
    global _OPENAI_CLIENT
    if not USE_LLM:
        return None
    if _OPENAI_CLIENT is None:
        from openai import OpenAI
        _OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)
    return _OPENAI_CLIENT

def llm_analyze(event: dict) -> str:
    client = _ensure_openai()
    if client is None:
        return "⚠️ OPENAI_API_KEY not set; LLM analysis unavailable."
    pretty = json.dumps(event, ensure_ascii=False, indent=2)
    system_msg = (
        "You are an SRE/Observability assistant. Given a JSON event with metrics/logs, "
        "produce concise, actionable recommendations. Focus on severity, key signals, "
        "likely causes, and next steps."
    )
    user_msg = (
        "Analyze the following event and respond in this format:\n"
        "Severity: <LOW|MODERATE|HIGH|CRITICAL>\n"
        "Signals: • <bullet 1>\n"
        "         • <bullet 2>\n"
        "Recommendations:\n"
        "1) <step 1>\n"
        "2) <step 2>\n"
        "3) <step 3>\n\n"
        "JSON:\n" + pretty
    )
    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
        temperature=OPENAI_TEMPERATURE,
        max_tokens=OPENAI_MAX_TOKENS,
    )
    return (resp.choices[0].message.content or "").strip()

def parse_llm_recos(text: str) -> List[str]:
    recos: List[str] = []
    for line in (text or "").splitlines():
        s = line.strip()
        if not s:
            continue
        if s[0].isdigit() and (") " in s or ". " in s):
            idx = s.find(") ")
            if idx == -1:
                idx = s.find(". ")
            if idx != -1:
                recos.append(s[idx+2:].strip())
    return recos

# -----------------------------
# GET /recommendations  (AUTH + pagination)
# -----------------------------
@app.get("/recommendations", summary="Fetch recent recommendations")#, dependencies=[Depends(require_api_key)])
def get_recommendations(
    page: int = Query(1, ge=1, description="Page number (1-based)"),
    page_size: int = Query(50, ge=1, le=200, description="Items per page"),
    since: Optional[str] = Query(None, description="ISO timestamp filter (optional)"),
    event_type: Optional[str] = Query(None, description="Filter by event type, e.g., system.cpu"),
):
    items, total = query_recommendations_paginated(
        page=page, page_size=page_size, since=since, event_type=event_type
    )
    pages = (total + page_size - 1) // page_size if page_size else 1
    return {
        "page": page,
        "page_size": page_size,
        "total": total,
        "pages": pages,
        "items": items,
    }

# -----------------------------
# POST /recommendations/analyze  (AUTH)
# -----------------------------
@app.post("/recommendations/analyze", summary="Analyze a single event JSON and return recommendations")#, dependencies=[Depends(require_api_key)])
def analyze_event(event: dict):
    """
    Accepts a single event in the contract schema:
      type, timestamp, resource, labels, metrics
    Returns recommendations from LLM if configured; otherwise from rules.
    """
    # Default timestamp if missing
    if "timestamp" not in event:
        event["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

    event = _normalize_metrics(event)
    et = event.get("type", "unknown.event")

    if USE_LLM:
        text = llm_analyze(event)
        recos = parse_llm_recos(text)
        return {
            "mode": "llm",
            "event_type": et,
            "recommendations_text": text,
            "recommendations": recos,
        }
    else:
        recos = evaluate_rules(event)
        return {
            "mode": "rules",
            "event_type": et,
            "recommendations": recos,
        }
